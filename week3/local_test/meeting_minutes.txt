
MEETING MINUTES
===============

üìù EXECUTIVE SUMMARY:
We have code changes in both the repos for the agent part of it, for the service layer. We have changes in changes in a particular on the service runner side, which takes care of the Java application. And then for every node app, like every nodeapp, we have made those changes on the, what do you say, the app.js so that at the time of load, the agent understand that it needs to,  monitor whatever call is going on.  logging would require a lot of changes at the server. We wanted to first bring that up and put it in production. For logging we need to need to have a specific file for every service. So we did not want to make that many changes to just enable data dog now. Port also highlighting is like at code level we want to get like if I. So a particular request is there at codelevel. How much each function or stack execution took the time it should  display the line as well. Plus we have wall time and CPU time per stack. So this kind of granular information if we have right then if a particular. request is too slow, we can drill down to the code itself. So do we have plan to enable this continuous profiling as well? Yeah, it it is an ad hoc request. There is a list of services for which we can enable this. splayed, not all services. We can extend this, but it depends on how like what all requirements. It was little like what we thought was we'll first start with web app, but we can include other services. Is there a way to selectively enable profiling within the service or it has to be at the service level? We don't need any service specific configuration change for this. It's an array which which says to Datadog that you have to include this in the service and the profiling.  integration for every service is done. Integration means it's a Java service. Then you will have a Java integration. It's a node. Yeah, it's done for all services. Sarup, you have anything else to discuss? I said,  this meeting for me now i thought we will discuss something because rishi shared a lot of documents to with me. The first thing that we would want to do is define the metric. There are certain metrics which are already available and then which we can see in the Datadog console set. And plus we have to define our own metrics for different things that we have for service for jobs. "We have not done it yet. going to build metrics around it. What kind of metrics are we looking for at this point? For job if we say let us say from the time I trigger till the time it gets picked up and then post that there are any wait periods between  different steps" So how based on job ID the further steps will be taken up like I just wanted to understand. So is it like via you will search that log ID in the logs or it will be a some something. Something will be configured that separate dashboard where we can see per job ID some statics or metrics. Sarubh: The problem always,  the difference in jobs is like we have a job start that's our API call and then as the pipeline is going on there can be multiple calls right disconnected calls for let's say job sta. nding it in a wrong way. We are planning to put some sort of identifier a  across all these distributed traces, top level traces, so that you can group everything together and sort of generate a timeline. So that's like a metric requirement, right? Not a case requirement. So we are not doing anything for metric here is what I wanted to very clearly specify. We have a lot of custom metrics for jobs already, but whatever exactly you're looking for, we'll have to see if it is already there or maybe we need to think of building it out. So that is what we are kind of trying  So what are metrics are currently available? Plus what is it that we would want to clearly define clearly identify or observe what's happening in our systems? The idea is that individual APM's individual services should be monitored by the domain owners. The SLASLO dashboard is being created basically the whole SLISLO error budgeting, which can be generated out of Datadog. And that will  will  be used to monitor individual services. There'll be cost implications and anything else that we might be better aware of. Not everything we will be able to create. be service-specific and individual services owners are responsible to make sure that they do not run out of error budget, right? And also make sure it's the right thresholds, not too conservative, et cetera. So basically it's Nathan Warn. We need a, that would be difficult for us, like, but in the lower environment, we will have read and write access. We can get that. In the dev and performance environment, if we can get, we can try out the things. Then maybe we can export the dashboard and import it in the Product later on.

üìã KEY DISCUSSION POINTS:
- Main topics covered in the meeting
- Important decisions and conclusions reached
- Significant information shared during the discussion

üìå NOTES:
This summary was generated from the meeting transcription using AI analysis.
For detailed information, please refer to the full transcription.

üïí MEETING DURATION: Approximately 19.1 minutes
(Based on average speaking pace of 150 words per minute)
        