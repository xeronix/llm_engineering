 We have code changes in both the repos for the agent part of it, for the service layer. We have changes in changes in a particular on the service runner side, which takes care of the Java application. And then for every node app, like every node app, we have made those changes on the, what do you say, the app.js so that at the time of load, the agent understand that it needs to,  monitor whatever call is going on so that we can go into dig deep we can dig deep of like from web app so because if you if you had not integrated that from web app we wouldn't have known which service is actually calling and getting you the values so which is called right so that's why we have so we have changes in both the places and we have complete coverage of the traces and also what is it browser level information like browser time etc that was not  nothing specific that we need to look and configure. It was covered as a part of the agent and initial set of. OK, and why? So why is logging not being covered till now? Like it's it's still in under discussion pipeline or if some we will do it. We will do it by this release. We wanted to first bring that up and put it in production because why did we do logging is for because logging would require a lot of changes at the server.  every service we need to make those changes before when we were including the agents only the service runner could changes would have taken care of it. But for logging we need to need to have a specific file for every service and via that we need to log in. So we did not want to make that many changes to just enable data dog now initially, but in the next one up we will put logging as well. For APM changes also we did for every service some specific change or it's it was kind of global change.  We did a global change for you. OK, and one more thing is like there is something called continuous profiling. Are we going to enable it? Because I was exploring data dog. Let me share my screen. Sure. So this is kind of my trial account. Are you able to see my screen?  Yes, yes, we are able to see it. Yeah, so this is kind of my trial account, not the alt tricks one. So if I see one of the sample service right? So one thing we would like to get through port also highlighting is like at code level we want to get like if I. So a particular request is there at code level. How much each function or stack execution took the time it should  display the line as well. Plus we have wall time and CPU time per stack. So this kind of granular information if we have right then if a particular request is too slow, we can drill down to the code itself. So do we have plan to enable this continuous profiling as well? Yeah, it it is an ad hoc request. We can do it. I think we have done it for the service layer. I think it was there before, but we it's a request that we need to like we need to have a discussion with them.  Because if I see. Where is it? Profiles right in APM here only I think web service web app gets displayed, not all services. Yeah, because yeah, so we did not do it for all services because we so there is a list of services for which we can enable this. There was an property we have just now included profile included only for web app. We can extend this, but it depends on how like what all requirements.  And it was little like what we thought was we'll first start with web app, but we can include the other services. I think Rishi, you can add a bit on this. Yeah, I think the initial discussion was that the cost of continuous profiling was high. There was some discussion to explore it first. So we kind of made a white list of services. So I just pinged the flag. You can search that and try it on. So you'll get that. And I think in Gconf only, web app was added to that list.  If any services needed, you can just add that for that particular environment will probably get profiling. Yeah. OK, alright so I mean. Just to clarify this particular flat profiling enabled service list. This is in G Corp. It's a common flag. Yeah. And we don't need any service specific configuration change for this. No, you just need to include the service name in that array. It's an array which which says to Datadog that you have to.  include this in the service and the profiling. OK, OK, got it, got it. Is there a this is just a question that I think is there a way to selectively enable profiling within the service or it has to be at the service level? Sorry, I didn't get your question. So when I enable profiling within a service, right? So does it have to be for  the complete service or is it also possible again in Datadog context is it also possible to profile specific parts of the service and not the full service just a question that I had I we need to explore that but currently the thing is that we need we can explore we have just enabled it for a particular thing but I have seen examples where they have tried but we have not tried it now but in Datadog it  but we have not tried it yet so. And integration for every service is done. Integration means it's a Java service. Then you will have a Java integration. It's a node. Yeah, it's done for all services. Yeah, it's done. It's done for all services. It is done at the service runner level, so every service that starts with the service runner is covered. But for all the node services we had to do it individually.  I think SaaS management service and VFS have been configured. Yeah, for those we made changes in the app.js of this file. Specifically for the node application was separate for Java layer. It was single file because the service. OK, got it. Let me check anything else. Sarup, you have anything else to discuss? I said,  this meeting for me now i thought we will discuss something because rishi shared a lot of documents to with me so a lot of information was there i could find out information there yeah you can share those dots across and then that would be helpful i just showed shared the nathan warner's roadmap uh recording with him yeah i i attended that meeting yeah that that i was aware of actually  I think this is good to start with. Swaroop, like Mathur will be working on any configuration change that will require or what? No, let us start planning. I want them to be involved. So most likely it might go there. But let us start planning and then we'll see. So what are the next steps that we are planning? So we have not yet defined every metric that we want to. The first thing that we would want to do is define the metric.  So there are certain metrics which are already available and then which we can see in the Datadog console set. And plus we have to define our own metrics for different things that we have for service for jobs. Are there any metrics related to jobs on Datadog at the moment or only services? It's an ask that has come. We can pick it up. We are going to pick that up.  Jira has already been created so that we can we are going to do it for job and other parameters. We are going to build metrics around it. Okay. We have not done it yet. What kind of metrics are we looking for at this point? For job if we say let us say from the time I trigger till the time it gets picked up and then post that there are any wait periods between  different steps and what time each step is taking. So those kind of details. Got it, got it. Yeah, that's exactly what got the mentioned. Yeah, I think some of that will be done when we enable the profiling, I guess. It's slightly different because what works for you was asking was at one place you get everything for one particular job ID, so you just so there is a parameter which you can configure and say job ID is equal to this.  and gets the whole cycle about that. But with profiling, you might be able to get the other information also. But yeah, this specific at one location that you will not get. Okay. But isn't that even that logs will be required? You will track it from the logs only, right? How you will do it. It's from the request you will pick up the job ID and then analyze it? Sorry? So like if we submit a  So how based on job ID the further steps will be taken up like I just wanted to understand. So is it like via you will search that log ID in the sorry job ID in the logs or it will be a some something. Something will be configured that separate dashboard where we can see per job ID some statics or metrics. Typically we should be able to look at it at job level. We should also be able to look at.  it across lots. If I want to see, again, all jobs are not equal, but at least wait times if you see as an example. The typical wait times for different jobs can be analyzed. That will be across jobs. But for individual jobs, again, at different type of connectors that's using different type of engine that's using. Those aggregations based on those metrics,  Those are the different things that can be done. Not just simply job ID, but it can be across also. Is that what you're asking people or am I understanding it in a wrong way? Yeah, I think I got it. Okay, one more question. Just to make sure that we are on the same page, right? So the one that Gautam was talking about is creating a distributed trace that maps everything together that will sort of include what you're looking for, but it's not exclusively  just for generating metrics, Sarubh, right? So basically, like fundamentally speaking, like the special behavior of a job is like, unlike the usual distributed trace we have is we have an origin request, let's say a request from UI goes to web app and then it ends up calling a bunch of other services which end up calling some other services as well, like the whole graph, right? All of them are connected by a single trace ID. So that's how it works. And that's what we get out of box. The problem always,  the difference in jobs is like we have a job start that's our API call and then as the pipeline is going on there can be multiple calls right disconnected calls for let's say job status polling and and then let's say somebody could do a cancellation as well but there are multiple different original requests that end up happening they all sort of loosely map to the same job right and that's the work that Gautam referred to we are planning to put some sort of identifier a  across all these distributed traces, top level traces, so that you can group everything together and sort of generate a timeline by just using that job ID or job group ID, like what all different requests happen and within them, what all are the different distributed traces involved. Yeah, I understood what Gautam's task was. I was just pointing out that on top of that, we would also want to do some additional aggregations. That's it.  Sure. So that's like a metric requirement, right? Not a case requirement. So we are not doing anything for metric here is what I wanted to very clearly specify. If you're looking for some metric, we'll have to think about it. Like, is that something that already exists? We have a lot of custom metrics for jobs already, but whatever exactly you're looking for, we'll have to see if it is already there or maybe we need to think of building it out. So the first phase of our planning, right? So that is what we are kind of trying  So what are metrics are currently available? Plus what is it that we would want to clearly define clearly identify or observe what's happening in our systems and how the performance is and then. OK, sure. One more thing is like network performance monitoring is not enabled, I guess because I think it would be important.  Java VFS services talking to external file system or some database side and there is some latency issue. We will be able to easily figure it out from that. Yeah, I think in general we're starting with a default set sort of, but for anything additional that we're looking for and basically there's a lot of work in flight already, but anything additional we're looking  we can make requests for that with the observability team, which is the central team that runs or manages Datadog for us. We can go to the observability channel and make such a request. There'll be cost implications and anything else that we might be better aware of, but they're like infra hooks and knobs that they can change for us. Hopefully, we don't have to test code at all for that. But yeah, fair ask. I think there have been a lot of things that we got them to enable.  I'm sure there will be a few more that will be useful so we can make a case. OK, and Santos are we planning to have like a separate every team is supposed to get separate dashboard here or what is? Do you have any idea? Yeah, so so the in general the idea is like individual APM's individual services should be monitored by the domain owners and only yesterday we were discussing about  SLASLO dashboard. That's like in POC right now, but yeah, that's being created basically the whole SLISLO error budgeting, which can be generated out of Datadog. And that will be service-specific and individual services owners are responsible to make sure that they do not run out of error budget, right? And also make sure that it's the right thresholds, not too conservative, et cetera. So yeah, that will be distributed to individual owners and they'll be  be responsible to you know sort of report or if they run out of error budget then justify why the thresholds need to be adjusted etc. So yeah, it will be distributed, but I think the core team or big people like Gotham and others and the data dog owners here they are responsible to set out the basic bare bones which others can use. But currently I think we only have read permissions, so I don't know whether we can create those dashboards if we want to try out some stuff or not.  Yeah, I think in general, not everything we will be able to create. So we can, again, if you need, if you have a need to get right access, then we can, again, everything goes via people in that channel. So basically it's Nathan Warner and Craig, forget his last name, but those are the people, Matt Wilson's team. Basically they are the people who own, so they can give us access or they can create dashboards for us. We just need to reach out to them.  Okay, sure. We'll try if we have any access issue, we can reach out to them. Just to add to this, there's only, so I don't, but I also reached out to the security teams for write access for the production thing, which it would be very difficult. We need a, that would be difficult for us, like, but in the lower environment, we will have read and write access. We can get that. In the dev and performance environment, if we can get, we can try out the things. Yeah, in the dev and in  In there you can get in there and yeah, there is a. Then maybe we can export the dashboard and import it in the. Product later on. So do you have anything to add? No nothing. OK, thank you everyone. Thank you for providing the valuable information. Thank you bye. Thanks a lot. Thank you.  you